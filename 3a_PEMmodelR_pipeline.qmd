---
title: "PEMmodelR_pipeline"
format: html
cache: TRUE
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Overall Model Workflow. 

To optimise the accuracy of modelling (and final map) for both forested and non-forested areas with the study area, we use a three part workflow to build the final map. We will build up to three separate components and then assemble the maps. This is required to both improve accuracy and address challenged in which set of covariates can be used to model forest vs non-forest mapunits. 

Broadly we will 
1)  Build a forest / non-forest model and map. This will provide a mask to differentiate areas  between forest and non-forest regions.

2) Build a forest model per BGC variant and assemble to a single forested layer. As this is the focus of PEM modelling this modelling component provides the most detailed model build parameters and additional methodology 

3) Build a non-forest model for the entire study area. This methodology is similar to forested workflow and can be replaces with other non-forest mapping products.


## Run the forest model

As the forest model is the primary research focus of this mapping process this will be the most detailed and require input in decision making to maximize the accuracy for intended purpose. 

## 1) Prepare training data and required inputs for modelling

The first step is to prepare the training data for the model. This includes the following steps:
1) convert prepared points to a csv file named training_pts.csv
2) generate a full covariate list as an RDS object

Note the attribute listed in the prep_tps function should be the attribute within the mapkey to which the raw fieldcalls will be matched. For forested maps this is typically the mapunit_ss_realm, however users can specify their own column if required.

If an error occurs is it worthwhile testing if the min_no is too low and increasing this value. A minimum of 20 appear to be a good starting point.
```{r}
#| eval: false
remotes::install_github("ninoxconsulting/PEMmodelr", ref = "prep_training_data", force = TRUE)
library(PEMr)
require(PEMmodelr)
source("./R/utils.R")
require(tictoc)
# library(tidyverse)
# library(sf)
# library(vip)
# require(terra)

```


```{r}
#| eval: false
#might want to generate a subfolder here but keeping it at root level for now

out_dir = fs::path(PEMprepr::read_fid()$dir_3020_draft$path_rel, "20_f")
covarkey = read.csv(fs::path(PEMprepr::read_fid()$dir_30_model$path_rel,"covar_key.csv"))


## 1.1 prep all training points

tpts <- PEMmodelr::prep_tps(
  allpts = sf::st_read(fs::path(PEMprepr::read_fid()$dir_20105030_attributed_field_data$path_rel, "allpoints_att.gpkg")),
  mapkey = read.csv(fs::path(PEMprepr::read_fid()$dir_30_model$path_abs,"mapunitkey_final.csv")),
  covarkey = covarkey,
  attribute = "mapunit_ss_realm",
  bec = sf::st_read(fs::path(PEMprepr::read_fid()$dir_1010_vector$path_rel, "bec.gpkg")),
  min_no = 20)


## generate a full list of covariates 

core_names  <- covarkey |>
  dplyr::filter(type == "core") |>
  dplyr::select(value) |> dplyr::pull()

mcols <- names(tpts)[!names(tpts) %in% c(core_names)]
saveRDS(mcols, fs::path(out_dir, "full_covariate_list.rds"))


# convert training points file to csv

tpts <- cbind(tpts, as.data.frame(sf::st_coordinates(tpts))) |>
  sf::st_drop_geometry()

mcols <- readRDS(fs::path(out_dir, "full_covariate_list.rds"))

reduced_vars <- reduce_features(mcols,
                                covar_dir = fs::path(PEMprepr::read_fid()$dir_1020_covariates$path_rel, "5m"),
                                covarkey = covarkey,
                                covtype = c("dem"),
                                cutoff = 0.90)

write.csv(reduced_vars, fs::path(out_dir, "reduced_covariate_list.csv"))
write.csv(tpts, fs::path(out_dir, "training_pts.csv"), row.names = FALSE)
#tpts <- read.csv( fs::path(out_dir, "training_pts.csv"))
tpts <- tpts |> dplyr::filter(!bgc_cat %in% c("ESSFmc", "ESSFmcw"))
model_bgc <- prep_model_tps(
  prepped_points = tpts,
  covars = reduced_vars,
  model_type = "f",
  outname = "model_input_pts.rds",
  out_dir = out_dir
)

``` 

```{r}
tune_bgc <- tune_model_params(
  prepped_points = model_bgc,
  covars = reduced_vars,
  min_no = 20,
  out_dir = out_dir,
  output_type = "best",
  accuracy_type = "roc_auc"
)

```

```{r}
#| eval: false

# designate the directory
out_dir = fs::path(PEMprepr::read_fid()$dir_3020_draft$path_rel, "20_f")

# read in data
bgc_pts_subzone <- readRDS(fs::path(out_dir, "model_input_pts.rds"))

# read in fuzzy matrix
fuzz_matrix <- read.csv(file.path(out_dir, "fuzzy_matrix.csv" )) |>
  dplyr::select(target, Pred, fVal)

# read in reduced covariate list 
covars = read.csv(fs::path(out_dir, "reduced_covariate_list.csv")) |> dplyr::pull()


## Run base model
run_base_model(
  bgc_pts_subzone = bgc_pts_subzone,
  fuzz_matrix = fuzz_matrix,
  covars = covars,
  use_neighbours = FALSE,
  detailed_output = TRUE,
  out_dir = out_dir)

```
```{r}
#| eval: false

out_dir = fs::path(PEMprepr::read_fid()$dir_3020_draft$path_rel, "20_f")
train_data <- readRDS(fs::path(out_dir, "model_input_pts.rds"))
covars <- read.csv(fs::path(out_dir, "reduced_covariate_list.csv")) |> dplyr::pull()

final_mod <- run_final_model(train_data,
                             covars,
                             model_bal = "base",
                             report = FALSE,
                             out_dir = out_dir)

```


```{r}
#| eval: false

model_dir = fs::path(PEMprepr::read_fid()$dir_3020_draft$path_rel, "20_f")
covars = read.csv(fs::path(model_dir, "reduced_covariate_list.csv")) |>  dplyr::pull()
cov_dir = fs::path(PEMprepr::read_fid()$dir_1020_covariates$path_rel, "5m")
tile_dir = fs::path(PEMprepr::read_fid()$dir_30_model$path_rel,"tiles")
bec_shp <- sf::st_read(fs::path(PEMprepr::read_fid()$dir_1010_vector$path_rel,"bec.gpkg"), quiet = TRUE)

run_predict_map(
  model_type = "f",
  model_dir = model_dir,
  model_name = "final_model_base.rds",
  covars = covars,
  cov_dir = cov_dir,
  tile_dir = tile_dir,
  bec_shp = bec_shp
)

```

##### OLD ######
```{r}
#| eval: false
#might want to generate a subfolder here but keeping it at root level for now

# out_dir = fs::path(PEMprepr::read_fid()$dir_3020_draft$path_rel, "20_f")
# cov.key <- PEMprepr::read_fid()$dir_30_model$path_abs
covarkey = read.csv(file.path(PEMprepr::read_fid()$dir_30_model$path_rel,"covar_key.csv"))


#generate_covar_key(allpts, overwrite = TRUE)

## 1.1 prep all training points

tpts <- PEMmodelr::prep_tps(
  allpts = sf::st_read(fs::path(PEMprepr::read_fid()$dir_20105030_attributed_field_data$path_rel, "allpoints_att.gpkg")),
  mapkey = read.csv(fs::path(PEMprepr::read_fid()$dir_30_model$path_abs,"mapunitkey_final.csv")),
  covarkey = covarkey,
  attribute = "mapunit_ss_realm",
  bec = sf::st_read(fs::path(PEMprepr::read_fid()$dir_1010_vector$path_rel, "bec.gpkg")),
  min_no = 20)


## generate a full list of covariates 

core_names  <- covarkey |>
  dplyr::filter(type == "core") |>
  dplyr::select(value) |> dplyr::pull()

mcols <- names(tpts)[!names(tpts) %in% c(core_names)]
saveRDS(mcols, fs::path(out_dir, "full_covariate_list.rds"))


# convert training points file to csv

tpts <- cbind(tpts, as.data.frame(sf::st_coordinates(tpts))) |>
  sf::st_drop_geometry()

write.csv(tpts, fs::path(out_dir, "training_pts.csv"), row.names = FALSE)

```
fuzzy matrix
```{r}
#| eval: false
#| 
# this is currently a placeholder that Claire is working on.
# it is possible to use the existing fuzzy_matrix.csv files until this is completed

#fuzzy_matrix <- read.csv(fs::path("fuzzy_matrix.csv" ))
model_input = read_fid()$dir_3010_inputs$path_abs
fuzzy_matrix <- read.csv(file.path(model_input, "fuzzy_matrix_basic_updated.csv" ))%>%
  dplyr::select(target, Pred, fVal)
utils::write.csv(fuzzy_matrix, fs::path(out_dir, "fuzzy_matrix.csv"), row.names = FALSE)

```

```{r}
#| eval: false

tpts <- read.csv( fs::path(out_dir, "training_pts.csv")) 
reduced_vars <- read.csv(file.path(model_input,  "reduced_covariate_list_dem_only.csv")) %>% dplyr::pull()
model_bgc <- prep_model_tps(
  prepped_points = tpts,
  covars = reduced_vars,
  model_type = "f",
  outname = "model_input_pts.rds",
  out_dir = out_dir
)

tune_bgc <- tune_model_params(
  prepped_points = model_bgc,
  covars = reduced_vars,
  min_no = 20,
  out_dir = out_dir,
  output_type = "best",
  accuracy_type = "roc_auc"
)


```

## 2) Run the base model


The model framework is based on an iterative leave-one-out process in which the accuracy estimates are generate by holding out one part of the data and then building and predicting with the other portion. Accuracy results are generated for each slice and uncertainty is estimated for the entire model. 
Accuracy is based on iteration by slice, or by site, where slice numbers are low. 
The rational for this is to reduce the need for a separate QA, and to improve uncertainty estimates of predictions. Each slice represents similar hypercube “space” and contains approximately 5 sites. For example stage 1 SBSmc2 at Deception includes 5 slices each with 5 sites = 25 sites or 50 triangles in total. 

This provides; 
1) confidence intervals around accuracy measures (based on 5 slices), 
2) minimize “data leakage” between the test and training sets, 
3) enables sites to be “blocked” to training and testing sets to prevent the model from training and testing on data from the same site (ie pair of transects). Cross validation is stratified by mapunit and grouped by site id. Test results are reported below.
4) To ensure an accurate assessment we calculate the average weighted means and standard deviation based on the number of transects per slice. This accounts for sampling structure where there are uneven number of sites per slice. 


The first step in model development is running a basic model where no balancing has been applied to the training points. This can provide a basic output, and can also be used to compare the impact of balancing to the basic model. The neighbors designates whether neighboring cells should be included in the model build. Detailed_outputs is not needed in this section, however will be used to calculate theta metrics in the next section. This model will generate a full accuracy assessment using the slices testing and training leave one out method as described above.   


```{r}
#| eval: false

tpts <- read.csv( fs::path(out_dir, "training_pts.csv")) #%>% mutate_if(is.character, as.factor)

model_bgc <- prep_model_tps(
  prepped_points = tpts,
  covars = reduced_vars,
  model_type = "f",
  outname = "model_input_pts.rds",
  out_dir = out_dir)

# designate the directory
out_dir = fs::path(PEMprepr::read_fid()$dir_3020_draft$path_rel, "20_f")

# read in data
bgc_pts_subzone <- readRDS(fs::path(out_dir, "model_input_pts.rds"))

# read in fuzzy matrix
fuzz_matrix <- read.csv(file.path(out_dir, "fuzzy_matrix.csv" )) |>
  dplyr::select(target, Pred, fVal)

# read in reduced covariate list 

covars <- read.csv(file.path(model_input,  "reduced_covariate_list_dem_only.csv")) %>% dplyr::pull()
# covars = read.csv(fs::path(out_dir, "reduced_covariate_list_dem_only.csv")) |> dplyr::pull()


## Run base model
run_base_model(
  bgc_pts_subzone = bgc_pts_subzone,
  fuzz_matrix = fuzz_matrix,
  covars = covars,
  use_neighbours = FALSE,
  detailed_output = TRUE,
  out_dir = out_dir)

```




```{r}
#| eval: false

#   path = "E:/PEM_AOIs"
#   aoi_name = "Deception_Lake"
#   #aoi_file = "D:/GitHub/PEM_pipeline/deception_aoi.gpkg"
#   open=FALSE
#   fid <- read_fid()
#   model_input = read_fid()$dir_3010_inputs$path_abs
# training_input = read_fid()$dir_1030_training_data$path_abs
```

# read in model parameters
```{r, eval = FALSE}

# fmat <- read.csv(file.path(model_input, "fuzzy_matrix_basic_updated.csv" ))%>%
#   dplyr::select(target, Pred, fVal)
# select reduced variables
reduced_vars <- read.csv(file.path(model_input,  "reduced_covariate_list_dem_only.csv")) %>% dplyr::pull()
nonfor_vars <- read.csv(file.path(model_input,  "reduced_covariate_list.csv")) %>% dplyr::pull()

bgc_pts_subzone <- readRDS(file.path(training_input, "model_input_pts.rds"))

extra_data <- readRDS(file.path(training_input, "extra_training_all.rds"))
best_tune <- fread(file.path(model_input, "best_tuning_dem_only.csv"))
mtry <- best_tune$mtry
min_n <- best_tune$min_n
map.key  <- read.csv("./_MapUnitLegend/MapUnitLegend.csv")

```

### run base model

```{r pressure, echo=FALSE}
model_draft = read_fid()$dir_3020_draft$path_abs

xx=1
model_bgc <- lapply(names(bgc_pts_subzone), function(xx) {
  
  xx <- names(bgc_pts_subzone[xx])
  
  alldat = bgc_pts_subzone[[xx]] %>% arrange(mapunit1)
  extradat = extra_data %>% filter(bgc == xx)
  outDir = file.path(model_draft, xx)
  
  dir.create(file.path(outDir, "raw_outputs"))
  detailed_outdir <- file.path(outDir, "raw_outputs")
 
  tdat <- alldat %>% mutate(slice = factor(slice))
  tdat <- tdat %>%
    dplyr::select(id, mapunit1, mapunit2, position,
      transect_id, tid, slice, any_of(reduced_vars))
  
  tdat <- tdat[complete.cases(tdat[, 8:length(tdat)]), ]
  
  train_data <- tdat 

  train_data <- droplevels(train_data)
  
  baseout <- run_base_model(
      train_data,
      fuzz_matrix = fmat,
      mtry = mtry,
      min_n = min_n,
      use.neighbours = TRUE, 
      detailed_output = FALSE, 
      out_dir = detailed_outdir)
  
  write.csv(baseout, file.path(detailed_outdir, "acc_base_results.csv"))
  
  ## generate model accuracy report
  #model_report(train_data, baseout, outDir)
  
})

```

generate models with various training set rebalancing
```{r}

bal_bgc <- lapply(names(bgc_pts_subzone), function(xx) {
  
  xx <- names(bgc_pts_subzone[xx])
  
  alldat = bgc_pts_subzone[[xx]]
  
  outDir = file.path(model_draft, xx)
  dir.create(file.path(outDir))
  #outDir <- file.path(outDir, "balance")
  
  tdat <- alldat %>% mutate(slice = factor(slice))
  tdat <- tdat %>%
    dplyr::select(id,mapunit1, mapunit2, position,
      transect_id, tid, slice, any_of(reduced_vars))
  
  tdat <- tdat[complete.cases(tdat[, 8:length(tdat)]), ]
  tdat <- droplevels(tdat)
  
  balance_optimisation_iteration(
      train_data = tdat,
      fuzz_matrix = fmat,
      ds_iterations = c(10,20,30,40,50,60),#,20,30),#10,20, multiples of least common class
      smote_iterations = c(0.1, 0.2),#, 0.6, 0.7,0.8, 0.9), percentage of dominant class
      mtry = mtry,
      min_n = min_n,
      use.neighbours = TRUE,
      out_dir = outDir,
      detailed_output = FALSE
  )
  
})

```

# Consolidate all balancing options and find optimum

```{r consolidate acc outputs and graph, echo = FALSE, fig.width=8, fig.height=8}

# combine all balance options into a single data table
acc_total <- foreach::foreach(xx = names(bgc_pts_subzone), .errorhandling = "pass",.combine = rbind) %do% {
  #k = data_list[2]
  # print(k)
  #xx = "ESSFmc"
  outDir = file.path(model_draft, xx)
  allacc <- combine_balance_ouputs(outDir)
  allacc <- allacc %>% dplyr::mutate(bgc = xx)
  
}

# select the best metrics for each Bec variant 

bgcs <- unique(as.factor(acc_total$bgc)) %>% droplevels()

best_balance <- foreach(b = levels(bgcs), .combine=rbind) %do% {
  #b <- bgcs[b]
  
  acc_bgc <- acc_total %>% dplyr::filter(bgc %in% b)
  best_metrics <- select_best_acc(acc_bgc) %>% 
    mutate(bgc = b)
  
  best_metrics
  
}

in_dir <- fid$model_inputs0310[2]

write.csv(best_balance, file.path(model_draft, "best_balancing_dem.csv"))


```

### Build final model for each BGC using best balance ratio
```{r}
model_draft = read_fid()$dir_3020_draft$path_abs
best_balance <- fread(file.path(model_draft, "best_balancing_dem.csv"))
model_final = read_fid()$dir_3030_final$path_abs
## select the balance otption 
#[1] "aspat_paf_theta.5" "aspat_paf_theta0" 
#[3] "aspat_paf_theta1"  "aspatial_sum"     
#[5] "spat_paf_theta.5"  "spat_paf_theta0"  
#[7] "spat_paf_theta1"   "spatial_sum"      
#[9] "overall"  

mbal <-"overall" 
#mbal <- "raw"

if(mbal== "raw") {
  
  mbaldf <- best_balance %>% dplyr::filter(maxmetric == "overall") %>%
  select(bgc, balance, ds_ratio, sm_ratio) %>%
  mutate(balance = "raw",
         ds_ratio = NA, 
         sm_ratio = NA)
} else {
  
  mbaldf <- best_balance %>% dplyr::filter(maxmetric == mbal) %>%
  select(bgc, balance, ds_ratio, sm_ratio) 

}

model_bgc <- lapply(names(bgc_pts_subzone), function(xx) {
  
  xx <- names(bgc_pts_subzone[xx])
  
  print(xx)
  alldat = bgc_pts_subzone[[xx]]

  outDir = file.path(model_final, xx)
  dir.create(file.path(outDir))
  final_data <- alldat %>%
    dplyr::filter(position == "Orig") %>%
    dplyr::select(mapunit1, any_of(reduced_vars))
  
  final_data <-  final_data[complete.cases(final_data[,2:length(final_data)]),]
  
  bgc_bal = mbaldf %>% filter(bgc == xx)
  ds_ratio = bgc_bal %>% pull(ds_ratio)
  sm_ratio = bgc_bal %>% pull(sm_ratio)
  
  final_model <- run_final_model(
      final_data,
      mtry = mtry,
      min_n = min_n,
      ds_ratio = ds_ratio, 
      sm_ratio = sm_ratio)
  
  # Output model 
  saveRDS(final_model, file.path(outDir, paste0("final_model",mbal,"_dem.rds")))

  # generate model accuracy report
 #final_model_report(bgc_bal, final_data, final_model, outDir)

})


```


```{r build tiles}

model_dir <- file.path(model_final)
bec_zones <- as.list(list.files(model_dir))

covdir <- read_fid()$dir_1020_covariates$path_abs
res_folder = "5m"

# select reduced variables
reduced_vars <- read.csv(file.path(read_fid()$dir_3010_inputs$path_abs,  "reduced_covariate_list_dem_only.csv")) %>% pull()

# get full raster list 
rast_list <- list.files(file.path(covdir, res_folder), pattern = ".tif$", full.names = TRUE)
rast_list <- rast_list[tolower(gsub(".tif", "", basename(rast_list))) %in% (reduced_vars)]
rstack <- terra::rast(rast_list)
template <- terra::rast(rast_list[2])
# generate tiles for mapping 

tile_dir <- file.path(model_dir, "tiles")

if(!dir.exists(file.path(tile_dir))){
  dir.create(file.path(tile_dir)) 
  # make tiles (might want to make NA tile )
  ntiles <- terra::makeTiles(template, 600, filename= file.path(tile_dir, "tile_.tif"),  na.rm=FALSE, overwrite = TRUE)
}else if(dir.exists(file.path(tile_dir))){
  ntiles <- list.files(tile_dir, full.names = T)
}



```

# loop through the bgcs and predict each tile for forest model

```{r}
covdir <- read_fid()$dir_1020_covariates$path_abs
res_folder = "5m"
model_final = read_fid()$dir_3030_final$path_abs
tile_dir = file.path(model_final, "tiles")
ntiles <- list.files(tile_dir, full.names = T)
rast_list <- list.files(file.path(covdir, res_folder), pattern = ".tif$", full.names = TRUE)
rast_list <- rast_list[tolower(gsub(".tif", "", basename(rast_list))) %in% (reduced_vars)]
rstack <- terra::rast(rast_list)

# for each bec zone
bgcs <- list.dirs(model_final, recursive = F,full.names = FALSE)
bgcs <- gsub("tiles", '', bgcs)

require(tictoc)
tic()
map_bgc <- for(b in bgcs){
  
  #b = bgcs[3]
  
  mfit = list.files(file.path(model_final, b), recursive = TRUE, pattern = "final_modeloverall_dem.rds", full.names = T)
  # mfit = mfiles[1]
  model <- readRDS(file.path(mfit))
  # make the output dir
  
  out_dir <- file.path(model_final, b,"map")
  # check if out_dir exists
  if(!dir.exists(file.path(out_dir))){ dir.create(file.path(out_dir))}
  
  predict_map(model, out_dir, tile_size = 600, tile_dir, rstack, probability = FALSE)
   
} 
toc()

# readRDS(file.path(model_dir, "ESSFmc","final_modeloverall_dem.rds")) %>% #workflows::pull_workflow_fit() %>% 
#    workflows::extract_fit_parsnip() %>%  vip::vip()
```
Merge mapunit keys

```{r merge mapunits modelled}
model_dir <- file.path(read_fid()$dir_3030_final$path_abs)
vector_path <- read_fid()$dir_0010_vector$path_abs
bec_shp <- st_read(file.path(vector_path,"bec.gpkg"), quiet = TRUE) %>%
   dplyr::select(MAP_LABEL)
aoi <- st_read(file.path(vector_path, "aoi.gpkg"), quiet = TRUE)
map.key  <- read.csv("./_MapUnitLegend/Deception_MapUnitLegend.csv")
folders <- as.list(c("ESSFmc", "ESSFmcw", "SBSmc2")) 

folders <- as.list(c("ESSFmc", "ESSFmcw", "SBSmc2")) 
# step 1:  set up a key for the combined map (includes all the units)
rkey <- lapply(folders, function (f){
  
  keys <- read.csv(file.path(model_dir, f, "map", "response_names.csv")) %>%
    mutate(model  = f)
})

rkey <- do.call("rbind", rkey)
rkey <- rkey %>% dplyr::mutate(map.response = seq(1:length(X)))

```

Merge the maps together 
```{r merge maps}
combo_map <- lapply(folders, function(f){
  
  # f <- folders[[1]]
  
  rtemp <- rast(file.path(model_dir, f, "map", "mosaic.tif"))
  
  rtemp[is.na(rtemp[])] <- 0 
  
  # filter to only predict over bgc
  bec_filter <- bec_shp %>%
    filter(MAP_LABEL == f) %>%
    dplyr::select(MAP_LABEL) 
  
  rtemp <- terra::mask(rtemp, bec_filter)
  
  subkey <- rkey %>% dplyr::filter(model == f) %>%
    mutate(mosaic = as.numeric(rownames(.)))
  
  # check if the key matches or needs reclassification 
  if (isTRUE(unique(subkey$mosaic == subkey$map.response))) {
    
    print("matching key")
    
  } else {
    
    print("updating key")
    
    m <- subkey %>%
      mutate(to = as.numeric(X), 
             from = as.numeric(X)+1) %>%
      dplyr::select(to, from, map.response) 
    
    reclm <-  as.matrix(m, ncol=3, byrow= TRUE)
    rtemp <-  terra::classify(rtemp, reclm, right = FALSE)#, include.lowest=TRUE)
    
  }
  
  rtemp <- terra::classify(rtemp, cbind(-Inf, 0, NA), include.lowest=TRUE)
  rtemp
  
})


# join all the maps together

if(length(folders) == 3) {
  
  all_map <- merge(combo_map[[1]], combo_map[[2]], combo_map[[3]])
 
} 

# all_key <- merge(combo_map[[1]], combo_map[[2]], overlap=TRUE)
# all_key <- merge(all_key, combo_map[[3]], overlap = TRUE)
# all_key <- merge(all_key, combo_map[[4]], overlap = TRUE)
# all_key <- merge(all_key, combo_map[[5]], overlap = TRUE)
# all_key <- merge(all_key, combo_map[[6]], overlap = TRUE)
# all_key <- merge(all_key, combo_map[[7]], overlap = TRUE)

# tidy key and output maps
rkey <- rkey %>% dplyr::select(map.response, model, X, .pred_class)

map_raster <- read_fid()$dir_4010_raster$path_abs

terra::writeRaster(all_map, filename = file.path(map_raster, "forest_mosaic_dem.tif"), overwrite = TRUE)

write.csv(rkey, file.path(map_raster, "response_combo_bcgs_key.csv"), row.names = FALSE)
```

build forest vs non-forest model using complete covariate set and just transect data
training data. Convert all site series to forest in transect data. Add in training points for clearcuts as forest points?, water (lakes), wetlands

```{r}
model_draft = read_fid()$dir_3020_draft$path_abs
best_balance <- fread(file.path(model_draft, "best_balancing_dem.csv"))
model_final = read_fid()$dir_3030_final$path_abs

covdir <- read_fid()$dir_1020_covariates$path_abs
res_folder = "5m"

map.key  <- read.csv("./_MapUnitLegend/MapUnitLegend_new.csv")
# select reduced variables
reduced_vars <- read.csv(file.path(read_fid()$dir_3010_inputs$path_abs,  "nonfor_covariate_list.csv")) %>% pull()

# get full raster list 
rast_list <- list.files(file.path(covdir, res_folder), pattern = ".tif$", full.names = TRUE)

rast_list <- rast_list[tolower(gsub(".tif", "", basename(rast_list))) %in% (reduced_vars)]
duplicated(rast_list)
#remove(rstack)
rstack <- terra::rast(rast_list)
xx <- names(rstack) %>% as.data.frame %>% rename(variable = 1) %>% count(variable)
## select the balance otption 
#[1] "aspat_paf_theta.5" "aspat_paf_theta0" 
#[3] "aspat_paf_theta1"  "aspatial_sum"     
#[5] "spat_paf_theta.5"  "spat_paf_theta0"  
#[7] "spat_paf_theta1"   "spatial_sum"      
#[9] "overall"  
map.key2 <- setDT(map.key)
mbal <-"overall" 
#mbal <- "raw"

# if(mbal== "raw") {
#   
#   mbaldf <- best_balance %>% dplyr::filter(maxmetric == "overall") %>%
#   select(bgc, balance, ds_ratio, sm_ratio) %>%
#   mutate(balance = "raw",
#          ds_ratio = NA, 
#          sm_ratio = NA)
# } else {
#   
#   mbaldf <- best_balance %>% dplyr::filter(maxmetric == mbal) %>%
#   select(bgc, balance, ds_ratio, sm_ratio) 
# 
# }
xx=1
model_bgc <- lapply(names(bgc_pts_subzone), function(xx) {
  
  xx <- names(bgc_pts_subzone[xx])
  
  print(xx)
  alldat = bgc_pts_subzone[[xx]]
  alldat <- setDT(alldat)
  alldat[map.key2, mapunit1 := MapUnit_nonforest_model, on = c("mapunit1" = "MapUnit_forest_model")]
  alldat[map.key2, mapunit2 := MapUnit_nonforest_model, on = c("mapunit2" = "MapUnit_forest_model")]
  

  outDir = file.path(model_final, xx)
  dir.create(file.path(outDir))
  final_data <- alldat %>%
    dplyr::filter(position == "Orig") %>%
    dplyr::select(mapunit1, any_of(nonfor_vars))
  
  final_data <-  final_data[complete.cases(final_data[,2:length(final_data)]),]
})
```
bring in extra training points
```{r}
training_dir <- read_fid()$dir_1030_training_data$path_abs
removed.units <- c("F", "FO", "A/FO", "FX", "PC")
extra_train <- st_read(file.path(training_dir,"r1_neighbours_att.gpkg"), quiet = TRUE) %>%
  filter(Position == "Orig", is.na(mapunit2), !mapunit %in% removed.units) %>% data.frame %>% dplyr::select(mapunit1, any_of(nonfor_vars))
xx <- bind_rows(final_data , extra_train)
##test
# extra_shp <- st_read(file.path(training_dir,"r1_neighbours_att.gpkg"), quiet = TRUE) %>%
#   filter(Position == "Orig", is.na(mapunit2), mapunit == "LA") %>% data.frame %>% select()
#   
# rast_list <- list.files(file.path(covdir, res_folder), pattern = ".tif$", full.names = TRUE)
# rast_list <- rast_list[tolower(gsub(".tif", "", basename(rast_list))) %in% (nonfor_vars)]
# extrastack <- terra::rast(rast_list)

```
# Build non-forest model
```{r}
# bgc_bal = mbaldf %>% filter(bgc == xx)
  # ds_ratio = bgc_bal %>% pull(ds_ratio)
  # sm_ratio = bgc_bal %>% pull(sm_ratio)
  ds_ratio = 50
  final_model <- run_final_model_WHM(
      final_data,
      extrarun = TRUE, 
      extradat = extrastack,
      mtry = mtry,
      min_n = min_n,
      ds_ratio = ds_ratio, 
      sm_ratio = sm_ratio)
  
  # Output model 
  saveRDS(final_model, file.path(outDir, paste0("final_model",mbal,"_nonfor.rds")))

  # generate model accuracy report
 #final_model_report(bgc_bal, final_data, final_model, outDir)

})


```

# loop through the bgcs and predict each tile for nonforest model

```{r}
covdir <- read_fid()$dir_1020_covariates$path_abs
res_folder = "5m"
model_final = read_fid()$dir_3030_final$path_abs
tile_dir = file.path(model_final, "tiles")
ntiles <- list.files(tile_dir, full.names = T)
rast_list <- list.files(file.path(covdir, res_folder), pattern = ".tif$", full.names = TRUE)
rast_list <- rast_list[tolower(gsub(".tif", "", basename(rast_list))) %in% (nonfor_vars)]
rstack <- terra::rast(rast_list)

# for each bec zone
bgcs <- list.dirs(model_final, recursive = F,full.names = FALSE)
bgcs <- gsub("tiles", '', bgcs)

require(tictoc)
tic()
map_bgc <- for(b in bgcs){
  
  #b = bgcs[3]
  
  mfit = list.files(file.path(model_final, b), recursive = TRUE, pattern = "final_modeloverall_nonfor.rds", full.names = T)
  # mfit = mfiles[1]
  model <- readRDS(file.path(mfit))
  # make the output dir
  
  out_dir <- file.path(model_final, b,"nonfor_map")
  # check if out_dir exists
  if(!dir.exists(file.path(out_dir))){ dir.create(file.path(out_dir))}
  
  predict_map(model, out_dir, tile_size = 600, tile_dir, rstack, probability = FALSE)
   
} 
toc()

# readRDS(file.path(model_dir, "ESSFmc","final_modeloverall_dem.rds")) %>% #workflows::pull_workflow_fit() %>% 
#    workflows::extract_fit_parsnip() %>%  vip::vip()
```
Merge nonfor mapunit keys

```{r merge mapunits modelled}
model_dir <- file.path(read_fid()$dir_3030_final$path_abs)
vector_path <- read_fid()$dir_0010_vector$path_abs
bec_shp <- st_read(file.path(vector_path,"bec.gpkg"), quiet = TRUE) %>%
   dplyr::select(MAP_LABEL)
aoi <- st_read(file.path(vector_path, "aoi.gpkg"), quiet = TRUE)
#map.key  <- read.csv("./_MapUnitLegend/Deception_MapUnitLegend.csv")
folders <- as.list(c("ESSFmc", "ESSFmcw", "SBSmc2")) 

folders <- as.list(c("ESSFmc", "ESSFmcw", "SBSmc2")) 
# step 1:  set up a key for the combined map (includes all the units)
rkey <- lapply(folders, function (f){
  
  keys <- read.csv(file.path(model_dir, f, "nonfor_map", "response_names.csv")) %>%
    mutate(model  = f)
})

rkey <- do.call("rbind", rkey)
rkey <- rkey %>% dplyr::mutate(map.response = seq(1:length(X)))

```

Merge the maps together 
```{r merge maps}
combo_map <- lapply(folders, function(f){
  
  # f <- folders[[1]]
  
  rtemp <- rast(file.path(model_dir, f, "nonfor_map", "mosaic.tif"))
  
  rtemp[is.na(rtemp[])] <- 0 
  
  # filter to only predict over bgc
  bec_filter <- bec_shp %>%
    filter(MAP_LABEL == f) %>%
    dplyr::select(MAP_LABEL) 
  
  rtemp <- terra::mask(rtemp, bec_filter)
  
  subkey <- rkey %>% dplyr::filter(model == f) %>%
    mutate(mosaic = as.numeric(rownames(.)))
  
  # check if the key matches or needs reclassification 
  if (isTRUE(unique(subkey$mosaic == subkey$map.response))) {
    
    print("matching key")
    
  } else {
    
    print("updating key")
    
    m <- subkey %>%
      mutate(to = as.numeric(X), 
             from = as.numeric(X)+1) %>%
      dplyr::select(to, from, map.response) 
    
    reclm <-  as.matrix(m, ncol=3, byrow= TRUE)
    rtemp <-  terra::classify(rtemp, reclm, right = FALSE)#, include.lowest=TRUE)
    
  }
  
  rtemp <- terra::classify(rtemp, cbind(-Inf, 0, NA), include.lowest=TRUE)
  rtemp
  
})


# join all the maps together

if(length(folders) == 3) {
  
  all_map <- merge(combo_map[[1]], combo_map[[2]], combo_map[[3]])
 
} 

# all_key <- merge(combo_map[[1]], combo_map[[2]], overlap=TRUE)
# all_key <- merge(all_key, combo_map[[3]], overlap = TRUE)
# all_key <- merge(all_key, combo_map[[4]], overlap = TRUE)
# all_key <- merge(all_key, combo_map[[5]], overlap = TRUE)
# all_key <- merge(all_key, combo_map[[6]], overlap = TRUE)
# all_key <- merge(all_key, combo_map[[7]], overlap = TRUE)

# tidy key and output maps
rkey <- rkey %>% dplyr::select(map.response, model, X, .pred_class)

map_raster <- read_fid()$dir_4010_raster$path_abs

terra::writeRaster(all_map, filename = file.path(map_raster, "nonforest_mosaic_dem.tif"), overwrite = TRUE)

write.csv(rkey, file.path(map_raster, "nonfor_response_combo_bcgs_key.csv"), row.names = FALSE)
```
